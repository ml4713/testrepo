---
title: "Housing Project - Rita Li"
output: html_notebook
---

## Business Understanding
This project task will be predicting the price for Australia, VIC state properties. We want to understand something....

Business context blah blah....

Also, thanks to Matt, we don't have data dictionary for this dataset which means we are building modeling on columns we have zero knowledge on and making educated guess.

```{r}
#1.set up the environment
rm(list = ls())
gc()

#1a. loading pkgs
pkg.list <- c("dplyr", 
              "tidyr", 
              "caret",
              "stringr",
              "lubridate",
              "dummies",
              "ggplot2",
              "randomForest",
              "caret",
              "data.table",
              "devtools",
              "sweep")

new.pkgs <- pkg.list[!pkg.list %in% installed.packages()[, "Package"]]

if (length(new.pkgs) > 0) {install.packages(new.pkgs, dependencies = TRUE)}
loaded.pkgs <- lapply(pkg.list, require, character.only = TRUE)

#1b. set up the work dir
setwd("/Users/trpff26/Desktop/misc/housing_project")
```


## Data Exploration
Now let's load the data and see what they looks like
```{r}
dat <- fread("housing_data.csv", header = TRUE, na.strings = c("", " ", "NA"))
head(dat)
```

```{r}
dim(dat)
```

We have 21 features and 17k data observations. Enough data for modeling. Nice.

Now, to understand the data better, let's take a closer look at some variables that need extra work before modeling.

Our target variable is one of the most important variables to check. And we found that it is heavily right skewed and has some missing values there. So let's take out the missing observations and log the price to make it normally distributed.
```{r}
# Price
dat[, Price := as.numeric(gsub('\\$|,','', trimws(Price)))]
dat <- dat[!is.na(Price)]
dat[, Price := log(Price)]
ggplot(dat, aes(x = Price)) + geom_density(color = "red", fill = "red", alpha = 0.3)
```

Having date as a feature is not informative. So extracting month and year from the date can introduce more granularity to the data. Hypothesis is that price varies seasonally and last year housing market might be differnt from this year (a little).

```{r}
#3b. Date, Year
dat[, Date := dmy(Date)]
dat[, Month := month(Date)]
dat[, Year := as.character(year(Date))]
```

Make sure categorical variables are factors.

```{r}
#3c. Postcode
dat[, Postcode := as.character(Postcode)]

#3d. factors 
factorCols <- names(dat)[sapply(dat, class) %in% "character"]
dat[, (factorCols) := lapply(.SD, as.factor), .SDcols = factorCols]
factorCols <- names(dat)[sapply(dat, class) %in% "factor"]
```

And after the cleaning step, the data now has better form and extra features to be fed into the mode.
```{r}
summary(dat)
```

Missing value is another important step in machine learning. Here we set the missing threshold to be 0.5 which means we remove any features that have more than 50% of values are not avaliable from this dataset.

```{r}
MISSING_THRES <- 0.5
```

And those featuers are:
```{r}
sort(sapply(dat, function(x) sum(is.na(x))/nrow(dat)), decreasing = TRUE)
```

So after removing those features, BuildingArea and YearBuilt specifically, we have 19 features left over. 
```{r}
dat.mr <- dat[, names(dat)[sapply(dat, function(x) sum(is.na(x))/nrow(dat)) < MISSING_THRES], with = FALSE]
sort(sapply(dat.mr, function(x) sum(is.na(x))/nrow(dat.mr)), decreasing = TRUE)
dim(dat.mr)
```

## Data visualization
Now the dataset is cleaned up and we are in good shape for visulization in which we are supposed to explore the relationship between target and potential features, among features themselves.


```{r}
# An example for continues
ggplot(dat.mr, aes(x = Distance)) + geom_density(color = "red", fill = "red", alpha = 0.3)
# An exmaple for categorical variables
ggplot(dat.mr, aes(x = Type)) + geom_bar()
# An exmaple for numerical summaries
summary(dat$Rooms)
```

And the second part is the multivariate analysis 

Median price for each month shows the trend that during summer, the median price is the lowest. And it starts to bouce back to the level compare to the beginning of the year.
```{r}
monthprice <- dat.mr[, .(med.price = median(Price, na.rm = TRUE)), by = c("Month")]
ggplot(monthprice, aes(x = Month, y = med.price)) + geom_line()
```
```{r}
ggplot(dat.mr[Rooms < 7], aes(x = as.factor(Rooms), y = Price)) + geom_boxplot()
```

Definitely, number of rooms closely relates the price and its area.

Also, does different seller have different preference towards what kind of houses they are more interested in? Top three sellers vs. Price:
```{r}
ggplot(dat.mr[SellerG %in% c("Jellis", "Nelson", "hockingstuart")], aes(x = SellerG, y = Price)) + geom_boxplot()

```


And the correlation study for numerical variables.
```{r}
# correlation analysis
cor(dat.mr$Distance, dat.mr$Rooms)
```


## Feature engineering

Obviously, we can't have categoricals with too many levels. Since it will slow down the training process as well as making the categorical variables stand out from other features.

Let's start with postcode which has 200 levels (crazy, and possibly overfit the model if it is too detailed.) What we can do is take the first two digits of the code and escalate to one level up comparing to the 4 digits postcode. Does it make sense to chop it off? Of course! Check out this link. 

```{r}
# document the decision and explain
# comment on the results
dat.mr[, Postcode := substring(as.character(Postcode), 1, 2)]
dat.mr[, Postcode := as.factor(Postcode)]
```

So now it looks much better with just two digits which decreases the unique level to 8. 


Seller name is another predictor that requires some transformation since we have 200 different seller. Should we question the predicting power of this feature? I think so but the graph below shows some interesting insights to answer this question.

```{r}
ggplot(dat.mr[SellerG %in% c("Jellis", "Nelson", "hockingstuart")], aes(x = SellerG, y = Price)) + geom_boxplot()
```

These are sellers with top selling records meaning top 3 most frequent sellers showed up in the dataset. For each of them, the property price distribution varies from each other. Maybe Nelson and hockingstuart (?) tend to have similar records but Jellis definitely obtains a more right skewed distribution for price. 

What about 20 of them whcih captures ~80% of the dataset? 

```{r}
topSellers <- sort(table(dat.mr$SellerG), decreasing = TRUE)
sum(topSellers[1:20])/nrow(dat.mr)
```
```{r}
topSellersName <- names(topSellers[1:20])
topSellersName
ggplot(dat.mr[SellerG %in% topSellersName], aes(x = SellerG, y = Price)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


It looks like it has some predictive power. So lets keep this feature for now. Wait, it's cardinality is pretty high. (262 levels). Right, one thing we can do, is keep the top 20 names and rename the rest of it to others. And now lets check if there is any other high-cardinal features.

```{r}
dat.mr[!SellerG %in% topSellersName, SellerG := "Others"]
sapply(dat.mr[, names(dat.mr) %in% factorCols, with = F], function(x) length(unique(x)))
summary(dat.mr)
```

Looks like we have more than one variables describe the location of the property. Suburb, Address, CoucilArea and Region name. Suburb is just another granularity for postcode. So we can remove suburb as its duplicating function. Address behaves like unique identifier so we can remove that as well. We have council area and region name, another way to summarize the location info. Keep it for now since we are using RF for modeling and correlated features are not a big trouble for those trees. (hopefully...)

```{r}
sort(table(dat.mr$CouncilArea))
ggplot(dat, aes(x = CouncilArea, y = Price)) + geom_boxplot() + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```




```{r}
# intuitively 
# base line model
# scientific method
dat.fr <- dat.mr[, !names(dat.mr) %in% c("Date", "Lattitude", "Longtitude", "Address", "Suburb"), with = FALSE]
sapply(dat.fr[, names(dat.fr) %in% factorCols, with = F], function(x) length(unique(x)))
```

Let's see the dimensionality after we dummy the cateogrical variables. But let's first fix the name for council area as they will become column names later.

```{r}
dat.fr[, CouncilArea := str_replace(CouncilArea, " ", "_")]
dat.fr[, Regionname := str_replace(Regionname, " ", "_")]
dat.fr[, Regionname := str_replace(Regionname, "-", "_")]

```


```{r}
# Dummy
dum.dat <- dummy.data.frame(dat.fr, sep = ".")
dum.dat <- dum.dat[complete.cases(dum.dat), ]
head(dum.dat)
dim(dum.dat)
```

Not bad for 10k data.
Alright, let's go ahead and start the most exciting part: Modeling!!!! 

Let's do 80% of training and 20% of testing. Among of 80% training, we will do a 10 fold cross validation for parameter tuning as well which means our training and validation set are

```{r}
set.seed(1015)
trainIndex <- createDataPartition(dum.dat$Price, p = .8, list = FALSE, times = 1)
houseTrain <- dum.dat[trainIndex, ]
houseTest <- dum.dat[-trainIndex, ]
dim(houseTrain)
dim(houseTest)
```

89-D for feature space. That'll work since Matt is pretty generous on data size that he gave me 17k data instead of 100 rows. 

## Modeling
Now we can start with a baseline model from RF. 
```{r}
rf.base <- randomForest(Price ~., data = houseTrain)
```

How many trees are needed?

```{r}
plot(rf.base)
```

Less than 100 is enough. 

```{r}
tuned.rf <- tuneRF(houseTrain[, !colnames(houseTrain) == "Price"], houseTrain$Price, ntreeTry = 100, stepFactor = 10, improve = 0.01)
```

```{r}
rf <- randomForest(Price ~., data = houseTrain, ntree = 100, mtry = 29)
```


## Testing
All set! Lets see how it works on the test data.
```{r}
pred <- predict(rf, houseTest) 
with(houseTest, mean((houseTest$Price - pred)^2)) 
```
Not bad! How aboue feature importance?

```{r}
varImpPlot(rf)
imp <- as.data.frame(importance(rf))
rownames(imp)[order(-imp$IncNodePurity)][1:10]
```

## How can we deploy this model?

1. Predict the price so you know if it is reasonable number for certain property.
2. The property type matters for the price (duh....) and distance to downtown area. So that may be another reflection on what customer really cares and could be the selling points when advertising for properties. For example, make it bold and 24 pts for front, etc. (I am not a designer, so that's all I know.)



## Next step
1. Of course, if time permits, more model techinques should be tested. For example, GBM, SVM or GLM could be more powerful than RF or even combine RF with GLM (feature selection first and then fit it into GLM).

2. Also, for other techinuqes, cross-validation would be more insightful comparing to RF since RF reports OOB which is equalivent to leave one out.

3. Always explore more features either internally (property's transaction history, how long it takes before its sold, etc.) or externally (crime rate, population density, any school in the neighborhood, etc) and make your model better.